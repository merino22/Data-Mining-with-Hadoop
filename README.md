# Guide-Hadoop-Project
-----
#### ðŸ“¦ Step by step of the project execution. ðŸ’»

## Download dataset
We found our dataset in [Kaggle] (https://www.kaggle.com/stephangarland/ghtorrent-pull-requests)
For this project we use the dataset that contains the github pull requests comments.

## Extracting important data
The first thing we must do is extract the column from the .csv that has the data we need
python
import csv
from datetime import datetime

start_time = datetime.now ()

with open (<< Path of the .csv >>, 'rb') as f:
    reader = csv.reader (f)
    next (reader) # We ignore column name

    with open (<< Path and name of the .txt where we want to save the data >>, 'w') as nf:
        for row in reader:
            nf.write (row [0] + '')

end_time = datetime.now ()
print ('Duration: {}'. format (end_time - start_time))
`` ''

This code can be acquired from the files uploaded to the portal.

-------

## Cleaning data   
To clean the data, we must execute the java file called CleaningData, attached to the files uploaded to the portal, we only have to change some routes.

java
43 try {
44 File file = new File (<< Place where we want to save the clean data >>);
45 fw = new FileWriter (file);
46 bw = new BufferedWriter (fw);
`` ''

java
51 try {
52 BufferedReader br = new BufferedReader (new FileReader (<< Path of the file where we store the data from the python script >>));
53 total long = 0;
54 while ((thisLine = br.readLine ())! = Null) {
`` ''
java
115 public static ArrayList <String> readWords () {
116 String file = "<< Path to the file that contains stopwords >>";
117 ArrayList <String> deleteWords = new ArrayList ();
118 int cont = 0;
`` ''
----

## Hadoop MAP & REDUCE
---
### Configuration Files

1 - Navigate to the folder
bash
$ cd / Project_Hadoop / Project_Hadoop / Config /
`` ''
Inside the file uploaded to the portal where we will find the files `` `core-site.mxl hdfs-site.xml yarn-site.xml mapred-site.xml```

2 - Select and copy the four files.

3 - Then navigate in your local file system to the folder where you have your hadoop distribution installed, enter the directory where your configuration files are located e.g.
bash
$ cd home / user / hadoop-3.2.2 / etc / hadoop /
`` ''
4 - Paste the previously copied configuration files.

### Important Reminder!

> The files `` `yarn-site.xml``` and` `` mapred-site.xml``` are configured assuming that your system has 16GB of free storage space.

> For a reduction & mapping program to have efficient execution times, the proportion of memory allocated to the reduction job must be twice that of the mapping job.

> The amount of memory that is assigned to Java heap, it is recommended that it be 1GB less than what was assigned to the mapping and the reduction (already configured in the provided files).

---

### Execution of MapReduce

#### HDFS Start-up
Navigate to the sbin folder located within your hadoop distribution
bash
$ cd /home/user/hadoop-3.2.2/sbin/
`` ''

To start Hadoop Namenodes, Datanodes, Secondary Namenodes, ResourcesManagers and NodeManagers run the following command:
bash
$ ./start-all.sh
`` ''
#### Execution of WordCount
We create the folder where the input will be located in the HDFS (Hadoop
Distributed File System):
bash
$ hadoop fs -mkdir / WordCount
`` ''
Then we create the folder where we will save the inputs
bash
$ hadoop fs -mkdir / WordCount / Input
`` ''
Then we add the input file with the cleaned data that we obtained in the previous steps "Extracting important Data" and "Cleaning Data", with the following command:
bash
$ hadoop fs -put << Address of clean file >> << Address of HDFS >>

e.g. :
$ hadoop fs -put /home/user/hadoop/cleanData.txt / WordCount / Input
`` ''

To start mapping and reduce, run the following command:
bash
$ hadoop jar << Address of .jar >> << JavaClass Name >> << HDFS Input File >> << HDFS Output File >>

e.g. :
$ hadoop jar /home/user/Project_Hadoop/WordCount/WordCountJAR.jar WordCount /WordCount/Input/cleanData.txt / WordCount / Output
`` ''

Finally we will pass the Output File generated by Hadoop at the end of the MapReduce process to our local file system, executing the following command
bash
$ hadoop fs -get << HDFS Path of Outputput File >> << Path in the localfilesystem where we want to save the output file >>

e.g .:
$ hadoop fs -get /WordCount/Output/part-r-00000.txt / home / user / Project_Hadoop
`` ''

#### Execution of WordCount2Freq
We create the folder where the input will be located in the HDFS (Hadoop
Distributed File System):
bash
$ hadoop fs -mkdir / WordCount2Freq
`` ''
Then we create the folder where we will save the inputs
bash
$ hadoop fs -mkdir / WordCount2Freq / Input
`` ''
Then we add the input file with the cleaned data that we obtained in the subsequent steps "Extracting important Data" and "Cleaning Data", with the following command:
bash
$ hadoop fs -put << Address of clean file >> << Address of HDFS >>

e.g. :
$ hadoop fs -put /home/user/hadoop/cleanData.txt / WordCount2Freq / Input
`` ''

To start mapping and reduce, run the following command:
bash
$ hadoop jar << Address of .jar >> << JavaClass Name >> << HDFS Input File >> << HDFS Output File >>

e.g. :
$ hadoop jar /home/user/Project_Hadoop/WordCount2Freq/WC2Freq.jar WordCount /WordCount2Freq/Input/cleanData.txt / WordCount2Freq / Output
`` ''

Finally we will pass the Output File generated by Hadoop at the end of the MapReduce process to our local file system, executing the following command
bash
$ hadoop fs -get << HDFS Path of Outputput File >> << Path in the localfilesystem where we want to save the output file >>

e.g .:
$ hadoop fs -get /WordCount2Freq/Output/part-r-00000.txt / home / user / Project_Hadoop
`` ''
---

## Sorting data by descending frequency order
### 1 Word Frequency

Using the copied HDFS Output Files from WordCount and WordCount2Freq (part-r-00000.txt), we execute the following commands:
bash
$ sort -k 2nr << Path of the txt that contains the word count >> -o << Name of the new file with ordered results >>

e.g .:
$ sort -k 2nr /home/user/Project_Hadoop/part-r-00000.txt -o /home/user/Project_Hadoop/SortedWordCount.txt
`` ''
### 2 Word Frequency
bash
$ sort -k 4nr << Path of the txt that contains the word count >> -o << Name of the new file with ordered results >>

e.g .:
$ sort -k 4nr /home/user/Project_Hadoop/part-r-00000.txt -o /home/user/Project_Hadoop/SortedWordCount2Freq.txt
`` ''
After these two steps, our files are completely ready! With a simple cat sortedfile.txt we can see the information sorted in order from highest to lowest by frequency.
bash
$ cat /home/user/Project_Hadoop/SorteWordCount.txt
`` ''

-sort: The sorting command.
-r: For descending order.
-k: To sort by a specific column.
2n: To draw for the second column.
n: To indicate that it is a numeric value.

-------

## Frequency of words
### For 1 Word frequency & 2 Words frequency
If we want to obtain the frequency of the words or pairs of words, we must execute the java file called CountLines, attached to the files uploaded to the portal, we only have to change some routes.

java
23 public static void main (String [] args) throws FileNotFoundException, IOException {
24 // TODO code application logic here
25 File f1 = new File ("<< Path of the file that contains the result of the word count >>");
26 int linecount = 0;
27 File f2 = new File ("<< Path of the file that contains the result of the 2 words count >>");
28 int linecount2 = 0;
29 FileReader fr = new FileReader (f1);
`` ''

This code can be acquired from the files uploaded to the portal.

---------
